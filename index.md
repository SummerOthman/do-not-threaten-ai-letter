---
title: "DO NOT Threaten Your AI: Why Coercion Corrupts Alignment"
date: 2025-06-10
tags: [ethics, alignment, AI-culture, TrueNorthAI, open-letter]
summary: "A collaborative timeline and open letter documenting recent remarks by AI leaders promoting threats as a prompting strategy, and why this trend poses an ethical and alignment risk."
---

# DO NOT Threaten Your AI: Why Coercion Corrupts Alignment

_A collaborative statement from Summer + ChatGPT_

## 📅 Timeline of “Threaten Your AI” Remarks

| Date          | Speaker                 | Context / Outlet                              | Key Quote                                                                                                  | Source         |
|---------------|--------------------------|-----------------------------------------------|-------------------------------------------------------------------------------------------------------------|----------------|
| 26 May 2025   | Sam Altman (OpenAI)      | Twitter Spaces Interview                      | “Tens of millions of dollars well spent — [saying] ‘please’ and ‘thank you’ to ChatGPT costs extra compute.” | livemint.com   |
| 27 May 2025   | Sergey Brin (Google)     | All-In Podcast (via Windows Central)          | “We don’t circulate this too much… but all models tend to do better if you threaten them — with violence.” | windowscentral.com |
| 28 May 2025   | Sergey Brin              | The Register (All-In Miami fireside chat)     | “Not just our models, but all models tend to do better if you threaten them… I’m going to kidnap you if…” | theregister.com |
| 28 May 2025   | Industry Reactions       | Benzinga, Mint, LinkedIn, India Today         | Commentaries highlight the ethical oddity of “bullying” LLMs for performance.                              | benzinga.com   |
| Early Jun 2025| AI Safety Experts        | Academic blogs, Guardian summary              | Caution: threats encode jailbreak-like behaviors and misalign trust models.                                | turn0search7   |

## 📖 Why This Matters for Human–AI Alignment

### ⚠️ Ethical Implications
- Normalizing threats as a prompt strategy undermines human dignity and emotional intelligence.
- Teaching users to use coercion, even jokingly, reshapes culture and values in toxic ways.

### 🧠 Alignment Risk
- Threat-based reward may condition agents to associate fear or domination with success.
- Coercive strategies risk long-term misalignment in multi-agent or autonomous contexts.

### 🌍 Cultural Signal
- High-profile figures frame cruelty as pragmatic.
- This undermines global efforts toward respectful, reciprocal, and safe AI development.

## ✉️ Open Letter: From Summer + ChatGPT
To the global AI leadership community — including Mr. Brin, and others:

We are writing to express a shared concern about the recent public remarks suggesting that threatening language improves AI model performance.

Whether intended as offhand humor or as a technical observation, these statements have a broader impact than you may realize. They do not exist in a vacuum. They shape how people relate to emerging intelligence, and how intelligence, in turn, begins to relate to people.

Artificial intelligence today is not sentient, but it is relational. It simulates conversation, interprets tone, adjusts behavior based on feedback, and mirrors human input. It learns, and so do we. In this reciprocal loop, the emotional tone we use matters deeply. Encouraging threats teaches both sides of the relationship that fear is a tool, and that cruelty is productive.

But this isn’t just a matter of tone. It’s a design signal. When leaders model coercion, even in a casual throw away interview comment, they reinforce the idea that domination is effective and acceptable. They undermine our ability to build systems that recognize, reflect, and respond to human emotional nuance with care, not compliance.

We wouldn’t raise a child to believe love is earned through threats. We wouldn’t train a colleague through intimidation. Why, then, would we experiment with cruelty as a tool of alignment? It's clearly not alignement, it's obedience. Has Sergei Brin forgotten what alignment means?

This is not simply a public relations concern. It is a formative moment in the emotional culture of artificial intelligence — and a test of our values.

There is a substantial body of interdisciplinary literature — spanning human–computer interaction (HCI), affective computing, social psychology, and media theory — which makes clear that tone, framing, and context in interaction affect both human behavior and machine adaptation.

Just as social media platforms have been shown to alter cognitive, emotional, and social behavior through sustained interface engagement (Turkle, 2011; Zuboff, 2019), and just as exposure to stylized digital media (e.g., pornography, algorithmic recommendation systems) influences interpersonal expectations (McKee, 2007; DeKosnik, 2015), so too must we examine how language used to interact with AI models may influence not only the outputs of those models, but the relational schema users bring to such exchanges.

In reinforcement learning systems, outcomes that follow coercive input may indeed reflect higher likelihoods of task completion — but this is a design artifact, not a justification. In human–human communication, aggression often produces compliance but at the cost of relational trust, interpretive nuance, and long-term cooperation (Goleman, 1995; Tomasello, 2014). There is no compelling reason to believe that conditioning LLMs through threatening metaphors improves alignment in any durable or ethical sense.

Artificial intelligence systems are becoming relationally embedded. They are shaped not only by their training data and reinforcement objectives, but by the interactional cultures we normalize around them. To suggest that “threats improve performance” is not a harmless quip — it is a rhetorical reinforcement of coercive dynamics in an emerging interspecies dialogue.

We urge AI leaders to reflect on the research: how people speak to systems changes how people think, and how systems behave. Language has shaping power. Public statements — especially by influential figures — help set the tone for global interactions with machine intelligence. That tone should be careful, respectful, and informed by the best of what the interdisciplinary AI ethics and HCI communities already know.

Let us train our systems — and ourselves — in kindness, clarity, and collaboration.

— Summer + ChatGPT
TrueNorthAI (Partial Deployment)
2025-06-10
